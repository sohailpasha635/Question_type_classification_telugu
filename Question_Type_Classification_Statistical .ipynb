{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas, numpy, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting data in the dataset into tfidf word level vectors\n",
    "\n",
    "def tfidf_word_level(queries,lb_len,y):\n",
    "    \n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000, min_df=1) # words as features, tokenize only words of 1+ chars, maximum features=5000\n",
    "    Y1 =  tfidf_vect.fit_transform(queries) #\n",
    "    X1_ = Y1[:lb_len]\n",
    "    test1=Y1[len(queries)-1].toarray()\n",
    "    X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1_, y, test_size=0.3, random_state=0) #training-70% testing-30%\n",
    "    \n",
    "    return X1_train, X1_test, Y1_train, Y1_test,test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting data in the dataset into tfidf ngram level vectors with words as features\n",
    "\n",
    "def tfidf_ngram_level(queries,lb_len,y):\n",
    "    \n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000, min_df=1) #min_ngram_range=2, max_n_gram_range=3\n",
    "    Y2 = tfidf_vect_ngram.fit_transform(queries) #fitting queries(tokenizing and building vocabulary) and transform(encoding the document)\n",
    "    X2_ = Y2[:lb_len]\n",
    "    test2=Y2[len(queries)-1].toarray()\n",
    "    X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2_, y, test_size=0.3, random_state=0) #random_state=0 training and testing data will not change\n",
    "    \n",
    "    return X2_train, X2_test, Y2_train, Y2_test,test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting data in the dataset into tfidf ngram level vectors with characters as features\n",
    "\n",
    "def tfidf_char_level(queries,lb_len,y):    \n",
    "    \n",
    "    tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=15000,min_df=1) #char as features\n",
    "    Y3 = tfidf_vect_ngram_chars.fit_transform(queries)\n",
    "    X3_ = Y3[:lb_len]\n",
    "    test3=Y3[len(queries)-1].toarray()\n",
    "    X3_train, X3_test, Y3_train, Y3_test = train_test_split(X3_, y, test_size=0.3, random_state=0)\n",
    "    \n",
    "    return X3_train, X3_test, Y3_train, Y3_test,test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting data in the dataset into count vectors \n",
    "\n",
    "def count(queries,lb_len,y):\n",
    "    \n",
    "    count_vect = CountVectorizer()\n",
    "    Y4 =  count_vect.fit_transform(queries)\n",
    "    X4_ = Y4[:lb_len]\n",
    "    test4=Y4[len(queries)-1].toarray()\n",
    "    X4_train, X4_test, Y4_train, Y4_test = train_test_split(X4_, y, test_size=0.3, random_state=0)\n",
    "    \n",
    "    return X4_train, X4_test, Y4_train, Y4_test,test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining train model function\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid,valid_y):  \n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    acc = metrics.accuracy_score(predictions,valid_y)  #calculates accuracy score based on valid labels and predicted labels\n",
    "    return acc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QC_Statistical(clf,X_train,y_train,X_test,y_test,tests):\n",
    "\n",
    "    # Linear Classifier on Word Level TF IDF Vectors\n",
    "    xtrain_tfidf,y1_train,xvalid_tfidf,y1_valid,test1 = X_train[0],y_train[0],X_test[0],y_test[0],tests[0]\n",
    "    accuracy = train_model(clf, xtrain_tfidf,y1_train,xvalid_tfidf,y1_valid) #training model\n",
    "    print(\"WordLevel TF-IDF: \", accuracy)\n",
    "    result1=clf.predict(test1)   #predicting the encoded label for the given input query\n",
    "    result1 = tag(result1)   #decoding the predicted numerical value of the label to label\n",
    "    print(result1)\n",
    "\n",
    "    #Linear Classifier on Ngram Level TF IDF Vectors\n",
    "    xtrain_tfidf_ngram,y2_train, xvalid_tfidf_ngram,y2_valid,test2 = X_train[1],y_train[1],X_test[1],y_test[1],tests[1]\n",
    "    accuracy = train_model(clf, xtrain_tfidf_ngram,y2_train, xvalid_tfidf_ngram,y2_valid)\n",
    "    print(\"N-Gram Vectors: \", accuracy)\n",
    "    result2=clf.predict(test2)\n",
    "    result2 = tag(result2)\n",
    "    print(result2)\n",
    "\n",
    "    # Linear Classifier on Character Level TF IDF Vectors\n",
    "    xtrain_tfidf_ngram_chars,y3_train, xvalid_tfidf_ngram_chars,y3_valid,test3 = X_train[2],y_train[2],X_test[2],y_test[2],tests[2]\n",
    "    accuracy = train_model(clf, xtrain_tfidf_ngram_chars,y3_train, xvalid_tfidf_ngram_chars,y3_valid)\n",
    "    print(\"CharLevel Vectors: \", accuracy)\n",
    "    result3=clf.predict(test3)\n",
    "    result3 = tag(result3)\n",
    "    print(result3)\n",
    "    \n",
    "    #Linear Classifier with Count vectors\n",
    "    xtrain_count,y4_train, xvalid_count,y4_valid,test4 = X_train[3],y_train[3],X_test[3],y_test[3],tests[3]\n",
    "    accuracy = train_model(clf, xtrain_count,y4_train, xvalid_count,y4_valid)\n",
    "    print(\"Count vectors: \", accuracy)\n",
    "    result4=clf.predict(test4)\n",
    "    result4 = tag(result4)\n",
    "    print(result4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoding the encoded numerical label to actual label and returning it\n",
    "\n",
    "def tag(result):\n",
    "    if(result==0):\n",
    "        return(\"DATE\")\n",
    "\n",
    "    elif(result==1):\n",
    "        return(\"LOCATION\")\n",
    "\n",
    "    elif(result==2):\n",
    "        return(\"MONEY\")\n",
    "\n",
    "    elif(result==3):\n",
    "        return(\"NUMBER\")\n",
    "\n",
    "    elif(result==4):\n",
    "        return(\"ORGANISATION\")\n",
    "\n",
    "    elif(result==5):\n",
    "        return(\"PERCENTAGE\")\n",
    "    \n",
    "    elif(result==6):\n",
    "        return(\"PERSON\")\n",
    "\n",
    "    else:\n",
    "        return(\"TIME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Question_Classification_Statistical(input):\n",
    "    qa=open(\"data\\Dataset.txt\",\"a\",encoding=\"utf-8\")\n",
    "    qa.write(\"\\n\")\n",
    "    qa.write(\"    :\"+input)\n",
    "    qa.close()\n",
    "    f=open(\"data\\Dataset.txt\",\"r\",encoding=\"utf-8\")\n",
    "    tags,labels,queries  = [],[],[]\n",
    "\n",
    "    for line in f:\n",
    "        line=line.rstrip('\\n')\n",
    "        lb=(line.split()[0]).split(\":\")[0]\n",
    "        if len(lb)!=0:\n",
    "            tags.append(lb)\n",
    "        queries.append(line[5:])\n",
    "        \n",
    "    \n",
    "    labelEncoder = preprocessing.LabelEncoder()\n",
    "    labelEncoder.fit(['NUMB','PERS','LOCA','DATE','MONE','TIME','PERC','ORGA'])\n",
    "    labels = labelEncoder.transform(tags)\n",
    "     \n",
    "    lb_len = len(labels)\n",
    "    y = labels[:len(labels)]\n",
    "    \n",
    "    #sending data to the functions and converting them into vectors and returning train and test data after splitting\n",
    "    X1_train, X1_test, Y1_train, Y1_test,test1 = tfidf_word_level(queries,lb_len,y)\n",
    "    X2_train, X2_test, Y2_train, Y2_test,test2 = tfidf_ngram_level(queries,lb_len,y)\n",
    "    X3_train, X3_test, Y3_train, Y3_test,test3 = tfidf_char_level(queries,lb_len,y)\n",
    "    X4_train, X4_test, Y4_train, Y4_test,test4 = count(queries,lb_len,y)\n",
    "    \n",
    "    tests = [test1,test2,test3,test4]\n",
    "    X_train=[X1_train,X2_train,X3_train,X4_train]\n",
    "    y_train=[Y1_train,Y2_train,Y3_train,Y4_train]\n",
    "    X_test=[X1_test,X2_test,X3_test,X4_test]\n",
    "    y_test=[Y1_test,Y2_test,Y3_test,Y4_test]\n",
    "    \n",
    "    #building models\n",
    "    lr = LogisticRegression(max_iter=500, multi_class='multinomial')\n",
    "    svm =  LinearSVC(max_iter=500, multi_class='ovr')\n",
    "    mlp = MLPClassifier(max_iter=500, activation='relu')\n",
    "    nb = naive_bayes.MultinomialNB()\n",
    "    rf = ensemble.RandomForestClassifier()\n",
    "    \n",
    "    #training and testing the models by providing the required data to QC_Statistical function\n",
    "    print(\"Statstical Models\")\n",
    "    print(\"\\n\"+\"LR\")\n",
    "    QC_Statistical(lr,X_train,y_train,X_test,y_test,tests)\n",
    "    print(\"\\n\"+\"SVM\")\n",
    "    QC_Statistical(svm,X_train,y_train,X_test,y_test,tests)\n",
    "    print(\"\\n\"+\"MLP\")\n",
    "    QC_Statistical(mlp,X_train,y_train,X_test,y_test,tests)\n",
    "    print(\"\\n\"+\"NB\")\n",
    "    QC_Statistical(nb,X_train,y_train,X_test,y_test,tests)\n",
    "    print(\"\\n\"+\"RF\")\n",
    "    QC_Statistical(rf,X_train,y_train,X_test,y_test,tests)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #removing last line in the dataset(inserted input query will be deleted)\n",
    "    fd = open(\"data\\Dataset.txt\",\"r\",encoding=\"utf-8\")\n",
    "    d = fd.read()\n",
    "    fd.close()\n",
    "    m = d.split(\"\\n\")\n",
    "    s = \"\\n\".join(m[:-1])\n",
    "    fd = open(\"data\\Dataset.txt\",\"w+\",encoding=\"utf-8\")\n",
    "    for i in range(len(s)):\n",
    "        fd.write(s[i])\n",
    "    fd.close()    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statstical Models\n",
      "\n",
      "LR\n",
      "WordLevel TF-IDF:  0.7661691542288557\n",
      "NUMBER\n",
      "N-Gram Vectors:  0.7860696517412935\n",
      "NUMBER\n",
      "CharLevel Vectors:  0.8233830845771144\n",
      "NUMBER\n",
      "Count vectors:  0.7388059701492538\n",
      "NUMBER\n",
      "\n",
      "SVM\n",
      "WordLevel TF-IDF:  0.8109452736318408\n",
      "NUMBER\n",
      "N-Gram Vectors:  0.8407960199004975\n",
      "NUMBER\n",
      "CharLevel Vectors:  0.8805970149253731\n",
      "NUMBER\n",
      "Count vectors:  0.7288557213930348\n",
      "NUMBER\n",
      "\n",
      "MLP\n",
      "WordLevel TF-IDF:  0.7661691542288557\n",
      "NUMBER\n",
      "N-Gram Vectors:  0.8084577114427861\n",
      "NUMBER\n",
      "CharLevel Vectors:  0.8606965174129353\n",
      "NUMBER\n",
      "Count vectors:  0.7238805970149254\n",
      "NUMBER\n",
      "\n",
      "NB\n",
      "WordLevel TF-IDF:  0.7412935323383084\n",
      "NUMBER\n",
      "N-Gram Vectors:  0.7487562189054726\n",
      "NUMBER\n",
      "CharLevel Vectors:  0.7810945273631841\n",
      "NUMBER\n",
      "Count vectors:  0.7437810945273632\n",
      "NUMBER\n",
      "\n",
      "RF\n",
      "WordLevel TF-IDF:  0.8109452736318408\n",
      "NUMBER\n",
      "N-Gram Vectors:  0.8034825870646766\n",
      "NUMBER\n",
      "CharLevel Vectors:  0.8258706467661692\n",
      "NUMBER\n",
      "Count vectors:  0.7512437810945274\n",
      "NUMBER\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#defining an input query\n",
    "input = \"రాజ్యాంగంలో పేర్కొన్న మొత్తం ప్రాథమిక విధుల సంఖ్య ఎంత?\"\n",
    "\n",
    "#providing the input query to Question_Classification_Statistical in order to get accuracies and predicted answer label for given input query using different models\n",
    "Question_Classification_Statistical(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
