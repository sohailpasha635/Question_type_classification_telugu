{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas, numpy, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn import model_selection\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading all the 300 dimensional vector representation of all the words present in pretrained fasttext wordembeddings in embeddings_index1 dictionary\n",
    "\n",
    "embeddings_index1 = {}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "for i, line in enumerate(open('cc.te.300.vec',encoding=\"utf-8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index1[values[0]] = np.asarray(values[1:], dtype='float32')  #words as key and 300 dimension vector values as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading all the 300 dimensional vector representation of all the words present in pretrained bytepair wordembeddings in embeddings_index2 dictionary\n",
    "\n",
    "embeddings_index2 = {}\n",
    "\n",
    "for i, line in enumerate(open('te.wiki.bpe.vs200000.d300.w2v.txt',encoding=\"utf-8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index2[values[0]] = np.asarray(values[1:], dtype='float32') #words as key and 300 dimension vector values as values   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating convolution neural network model\n",
    "\n",
    "def create_cnn(input_size,vocab_size,embedding_matrix):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_size, ))  #defining the input format\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(vocab_size, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.25)(embedding_layer) #entire feature maps from the convolutional layer which are then not used during pooling are droped out\n",
    "\n",
    "    #Add the convolutional layer\n",
    "    conv_layer = layers.Convolution1D(256, 3, activation=\"tanh\")(embedding_layer) #256: no of filters, 3:kernal size \n",
    "    \n",
    "    #Add the pooling layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer) #selects the maximum of the values in the input feature map region\n",
    "    \n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(128, activation=\"tanh\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1) #the outputs of a layer under dropout are randomly subsampled\n",
    "    output_layer2 = layers.Dense(8, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)  #models define how to organize (uses Functional API Model)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_lstm(input_size,vocab_size,embedding_matrix):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_size, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(vocab_size, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.25)(embedding_layer)\n",
    "\n",
    "    \n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "    \n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(128, activation=\"tanh\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(8, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_gru(input_size,vocab_size,embedding_matrix):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_size, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(vocab_size, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.25)(embedding_layer)\n",
    "\n",
    "    \n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "    \n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(128, activation=\"tanh\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(8, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_bi(input_size,vocab_size,embedding_matrix):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_size, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(vocab_size, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.25)(embedding_layer)\n",
    "\n",
    "    \n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "    \n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(128, activation=\"tanh\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(8, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QC_Embedding(x_train,x_valid,vocab_size,input_size,embedding_matrix,input_query,encoder,token):\n",
    "    \n",
    "    train_seq_x,trainLabels,valid_seq_x,validLabels = x_train[0],x_train[1],x_valid[0],x_valid[1] \n",
    "    \n",
    "    \n",
    "    cnn = create_cnn(input_size,vocab_size,embedding_matrix)                \n",
    "    cnn.fit(train_seq_x, trainLabels, epochs=20, verbose=0)  #training the model with the provided data\n",
    "    predictions = cnn.predict(valid_seq_x)\n",
    "    # print(predictions)\n",
    "    predictions1 = predictions.argmax(axis=-1) #gives max value in target function\n",
    "    validLabels1 = validLabels.argmax(axis=-1)\n",
    "    # print(predictions1)\n",
    "    acc = metrics.accuracy_score(predictions1, validLabels1)\n",
    "    print(\"CNN Model: \",acc)\n",
    "    valid_seq1 = sequence.pad_sequences(token.texts_to_sequences(input_query),maxlen=32) #padding sentences to ensure same length for all sentences\n",
    "    predict=cnn.predict(valid_seq1)\n",
    "    predict = predict.argmax(axis=-1)\n",
    "    print(encoder.inverse_transform(predict))\n",
    "    \n",
    "    \n",
    "    rnn_lstm = create_rnn_lstm(input_size,vocab_size,embedding_matrix)\n",
    "    rnn_lstm.fit(train_seq_x, trainLabels, epochs=20, verbose=0) #verbose=0 gives no info for every epoch\n",
    "    predictions_lstm = rnn_lstm.predict(valid_seq_x)\n",
    "    predictions1_lstm = predictions_lstm.argmax(axis=-1)\n",
    "    acc2 = metrics.accuracy_score(predictions1_lstm, validLabels1) #returns the accuracy based on predicted and valid labels\n",
    "    print(\"\\nRNN LSTM Model: \",acc2)\n",
    "    predict2=rnn_lstm.predict(valid_seq1)\n",
    "    predict2 = predict2.argmax(axis=-1)\n",
    "    print(encoder.inverse_transform(predict2))\n",
    "       \n",
    "    \n",
    "    rnn_bi = create_rnn_bi(input_size,vocab_size,embedding_matrix)\n",
    "    rnn_bi.fit(train_seq_x, trainLabels, epochs=20, verbose=0)\n",
    "    predictions_bi = rnn_bi.predict(valid_seq_x)\n",
    "    predictions1_bi = predictions_bi.argmax(axis=-1)\n",
    "    acc3 = metrics.accuracy_score(predictions1_bi, validLabels1)\n",
    "    print(\"\\nRNN BI LSTM Model: \",acc3)\n",
    "    predict3=rnn_bi.predict(valid_seq1)\n",
    "    predict3 = predict3.argmax(axis=-1)\n",
    "    print(encoder.inverse_transform(predict3))\n",
    "    \n",
    "    \n",
    "    rnn_gru = create_rnn_gru(input_size,vocab_size,embedding_matrix)\n",
    "    rnn_gru.fit(train_seq_x, trainLabels, epochs=20, verbose=0)\n",
    "    predictions_gru = rnn_gru.predict(valid_seq_x)\n",
    "    predictions1_gru = predictions_gru.argmax(axis=-1)\n",
    "    acc4 = metrics.accuracy_score(predictions1_gru, validLabels1)\n",
    "    print(\"\\nRNN GRU Model: \",acc4)\n",
    "    predict4=rnn_gru.predict(valid_seq1)\n",
    "    predict4 = predict4.argmax(axis=-1)\n",
    "    print(encoder.inverse_transform(predict4))  #decodes the encoded predicted label and return the actual label value\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Question_Classification_DL(input_query):\n",
    "    f=open(\"data\\Dataset.txt\",\"r\",encoding=\"utf-8\")\n",
    "    tags,queries  = [],[]\n",
    "\n",
    "    for line in f:\n",
    "        line=line.rstrip('\\n')\n",
    "        lb=(line.split()[0]).split(\":\")[0]\n",
    "        if len(lb)!=0:\n",
    "            tags.append(lb)\n",
    "        queries.append(line[5:])\n",
    "        \n",
    "    #queries contains questions , tags contains the labels for the queries    \n",
    "    trainDF = pd.DataFrame()\n",
    "    trainDF['text'] = queries\n",
    "    trainDF['label'] = tags\n",
    "\n",
    "\n",
    "    #splitting the text,labels into train,valid  with default ration 75:25  \n",
    "    #train 75%  valid 25%\n",
    "\n",
    "    train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'], random_state=10)\n",
    "\n",
    "\n",
    "    #converting the train,valid labels to one-hot encoding using labelEncoder\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    trainLabels = encoder.fit_transform(train_y)\n",
    "    trainLabels = [np_utils.to_categorical(i, num_classes=8) for i in trainLabels]\n",
    "    trainLabels = np.asarray(trainLabels)\n",
    "\n",
    "    validLabels = encoder.fit_transform(valid_y)\n",
    "    validLabels = [np_utils.to_categorical(i, num_classes=8) for i in validLabels]\n",
    "    validLabels = np.asarray(validLabels)\n",
    "    \n",
    "    token = text.Tokenizer()\n",
    "    token.fit_on_texts(trainDF['text'])\n",
    "    word_index = token.word_index\n",
    "    vocab_size =len(word_index)+1\n",
    "    \n",
    "    input_size=32\n",
    "    \n",
    "    train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x),maxlen=input_size) #padding sentences to ensure same sentence length\n",
    "    valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x),maxlen=input_size)\n",
    "\n",
    "\n",
    "    embedding_matrix1 = np.zeros((len(word_index)+1, 300))\n",
    "    embedding_matrix2= np.zeros((len(word_index)+1, 300))\n",
    "\n",
    "    for word,i in word_index.items():\n",
    "            embedding_vector = embeddings_index1.get(word)    # checking that particular indexed word in telugu embedding .vec file\n",
    "            if embedding_vector is not None:                 # if it is found in that .vec file  \n",
    "                embedding_matrix1[i] = embedding_vector\n",
    "                \n",
    "    for word,i in word_index.items():\n",
    "        embedding_vector = embeddings_index2.get(word)    # checking that particular indexed word in telugu embedding .vec file\n",
    "        if embedding_vector is not None:                 # if it is found in that .vec file  \n",
    "            embedding_matrix2[i] = embedding_vector           \n",
    "    \n",
    "    x_train =[train_seq_x,trainLabels]\n",
    "    x_valid =[valid_seq_x,validLabels]\n",
    "    \n",
    "    print(\"With Deep learning Techniques\")\n",
    "    print(\"\\n\")\n",
    "    print(\"Embedding using FastText Pre trained Embedding\")\n",
    "    QC_Embedding(x_train,x_valid,vocab_size,input_size,embedding_matrix1,input_query,encoder,token)\n",
    "    print(\"\\n\")\n",
    "    print(\"\\nEmbedding using Byte-Pair Encoding Pre trained Embedding\")\n",
    "    QC_Embedding(x_train,x_valid,vocab_size,input_size,embedding_matrix2,input_query,encoder,token)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Deep learning Techniques\n",
      "\n",
      "\n",
      "Embedding using FastText Pre trained Embedding\n",
      "CNN Model:  0.8776119402985074\n",
      "['LOCA']\n",
      "\n",
      "RNN LSTM Model:  0.8776119402985074\n",
      "['LOCA']\n",
      "\n",
      "RNN BI LSTM Model:  0.9014925373134328\n",
      "['LOCA']\n",
      "\n",
      "RNN GRU Model:  0.9014925373134328\n",
      "['LOCA']\n",
      "\n",
      "\n",
      "\n",
      "Embedding using Byte-Pair Encoding Pre trained Embedding\n",
      "CNN Model:  0.8686567164179104\n",
      "['LOCA']\n",
      "\n",
      "RNN LSTM Model:  0.8537313432835821\n",
      "['LOCA']\n",
      "\n",
      "RNN BI LSTM Model:  0.8656716417910447\n",
      "['LOCA']\n",
      "\n",
      "RNN GRU Model:  0.8507462686567164\n",
      "['LOCA']\n"
     ]
    }
   ],
   "source": [
    "input = ['మద్యప్రదేశ్ రాజధాని ఏమిటి?']\n",
    "\n",
    "Question_Classification_DL(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
